{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EwHtSKFDjJG",
        "outputId": "32fe09ed-ab51-4bbb-face-ed441e8c2e5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Archive:  /content/drive/MyDrive/t5eval_stdev6.zip\n",
            "replace t5eval_contrastive/training_args.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace t5eval_contrastive/tokenizer.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace t5eval_contrastive/tokenizer_config.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace t5eval_contrastive/spiece.model? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace t5eval_contrastive/special_tokens_map.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace t5eval_contrastive/pytorch_model.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import T5EncoderModel, T5Tokenizer, AutoTokenizer, PreTrainedModel # Added this import\n",
        "from datasets import load_dataset, Dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "from scipy.stats import spearmanr\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import gc\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Monta Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "!unzip /content/drive/MyDrive/t5eval_stdev6.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j5ySTGq_Hn6B"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class T5EncoderForRegression(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "        self.encoder = T5EncoderModel.from_pretrained(model_name)\n",
        "        hidden_size = self.encoder.config.d_model\n",
        "\n",
        "        # Regressor pi√π profondo\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.LayerNorm(hidden_size // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.15),\n",
        "            nn.Linear(hidden_size // 2, 64),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        # Inizializzazione\n",
        "        for module in self.regressor.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.config = self.encoder.config\n",
        "        self.config.problem_type = \"regression\"\n",
        "        self.config.num_labels = 1\n",
        "        self.forward_count = 0\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        labels=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "\n",
        "        self.forward_count += 1\n",
        "        # Encoder forward\n",
        "        outputs = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        # Mean pooling with safety checks\n",
        "        last_hidden = outputs.last_hidden_state  # [batch, seq_len, hidden]\n",
        "        # Debug: Check for NaN in encoder output\n",
        "        if self.forward_count <= 3 or torch.isnan(last_hidden).any():\n",
        "            print(f\"\\nüîç Forward pass #{self.forward_count}\")\n",
        "            print(f\"  Encoder output shape: {last_hidden.shape}\")\n",
        "            print(f\"  Encoder output has NaN: {torch.isnan(last_hidden).any()}\")\n",
        "            print(f\"  Encoder output min/max: {last_hidden.min().item():.4f}/{last_hidden.max().item():.4f}\")\n",
        "\n",
        "        mask = attention_mask.unsqueeze(-1).float()  # [batch, seq_len, 1]\n",
        "\n",
        "        # Prevent division by zero\n",
        "        mask_sum = mask.sum(dim=1)\n",
        "        mask_sum = torch.clamp(mask_sum, min=1e-9)\n",
        "\n",
        "        # Weighted average by attention mask\n",
        "        pooled = (last_hidden * mask).sum(dim=1) / mask_sum\n",
        "\n",
        "        # Debug pooled output\n",
        "        if self.forward_count <= 3 or torch.isnan(pooled).any():\n",
        "            print(f\"  Pooled output shape: {pooled.shape}\")\n",
        "            print(f\"  Pooled has NaN: {torch.isnan(pooled).any()}\")\n",
        "            print(f\"  Pooled min/max: {pooled.min().item():.4f}/{pooled.max().item():.4f}\")\n",
        "            print(f\"  Regressor weight stats: mean={self.regressor[1].weight.mean().item():.4f}, std={self.regressor[1].weight.std().item():.4f}\")\n",
        "        # Regression head\n",
        "        logits = self.regressor(pooled).squeeze(-1)  # [batch]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # Ensure labels are float and handle NaN values\n",
        "            labels = labels.float()\n",
        "            valid_mask = ~torch.isnan(labels) & ~torch.isinf(labels)\n",
        "\n",
        "            if valid_mask.sum() > 0:\n",
        "                loss = F.mse_loss(logits[valid_mask], labels[valid_mask])\n",
        "            else:\n",
        "                # If no valid labels, create a dummy loss\n",
        "                loss = torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
        "                print(f\"  ‚ö†Ô∏è No valid samples in batch!\")\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"logits\": logits\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920,
          "referenced_widgets": [
            "f8c82d5d221f44c4b051a0d4679f31ec",
            "99bc0bdd80eb4b6fb22a34406ce96696",
            "724f7d0d1b09424bb0cb6aca69164261",
            "9a34c017f7884302b6b1efc94f3a685f",
            "19f5af85be624530a572b9f771924ae1",
            "bbbccdfa00014c75bdf00904f020f04f",
            "f6c193e1b31e40c6b068490c427e6b2a",
            "ae9249c57f254b33bf71f368ea911d86",
            "f0c9c6784089401b89459cb4c57c7cb7",
            "3ef7c3b513e5471abe8ca55cbb6fb635",
            "36f4bbb8dbb44a6e8e9edb58b8a9b0a2"
          ]
        },
        "id": "D-KRqLctDNY5",
        "outputId": "e1018179-6cf6-48a0-97f0-286e1bf2d7d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîÑ Ricaricamento modello salvato...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8c82d5d221f44c4b051a0d4679f31ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Modello ricaricato con successo!\n",
            "\n",
            "üîÆ Generazione predizioni sul test set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rGenerating predictions:   0%|          | 0/930 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Forward pass #1\n",
            "  Encoder output shape: torch.Size([1, 320, 2048])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rGenerating predictions:   0%|          | 1/930 [00:01<22:20,  1.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Encoder output has NaN: False\n",
            "  Encoder output min/max: -0.7748/0.8690\n",
            "  Pooled output shape: torch.Size([1, 2048])\n",
            "  Pooled has NaN: False\n",
            "  Pooled min/max: -0.2943/0.3215\n",
            "  Regressor weight stats: mean=0.9962, std=0.0105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rGenerating predictions:   0%|          | 2/930 [00:01<11:24,  1.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Forward pass #2\n",
            "  Encoder output shape: torch.Size([1, 320, 2048])\n",
            "  Encoder output has NaN: False\n",
            "  Encoder output min/max: -0.8758/0.8318\n",
            "  Pooled output shape: torch.Size([1, 2048])\n",
            "  Pooled has NaN: False\n",
            "  Pooled min/max: -0.2783/0.2963\n",
            "  Regressor weight stats: mean=0.9962, std=0.0105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rGenerating predictions:   0%|          | 3/930 [00:01<07:58,  1.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Forward pass #3\n",
            "  Encoder output shape: torch.Size([1, 320, 2048])\n",
            "  Encoder output has NaN: False\n",
            "  Encoder output min/max: -0.7592/0.7900\n",
            "  Pooled output shape: torch.Size([1, 2048])\n",
            "  Pooled has NaN: False\n",
            "  Pooled min/max: -0.2628/0.3158\n",
            "  Regressor weight stats: mean=0.9962, std=0.0105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 930/930 [04:22<00:00,  3.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ File predictions.jsonl creato con successo con 930 predizioni!\n",
            "üìä Range predizioni: [1.99, 4.01]\n",
            "\n",
            "üîç Prime 5 predizioni:\n",
            "  ID: 0, Prediction: 4.0100\n",
            "  ID: 1, Prediction: 2.4718\n",
            "  ID: 2, Prediction: 4.0100\n",
            "  ID: 3, Prediction: 2.7912\n",
            "  ID: 4, Prediction: 4.0100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===========================\n",
        "# 1. RICARICA MODELLO SALVATO\n",
        "# ===========================\n",
        "print(\"\\nüîÑ Ricaricamento modello salvato...\")\n",
        "model_name = \"google/flan-t5-xl\"  # Torniamo a base\n",
        "# Ricostruisci l'architettura\n",
        "model = T5EncoderForRegression(model_name)\n",
        "tokenizer= AutoTokenizer.from_pretrained('t5eval_contrastive')\n",
        "# Riapplica LoRA con gli stessi parametri del training\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.FEATURE_EXTRACTION,\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.2,\n",
        "    target_modules=[\"q\", \"v\", \"k\", \"o\"],\n",
        "    bias=\"none\",\n",
        "    inference_mode=True,\n",
        "    modules_to_save=[\"regressor\"]\n",
        ")\n",
        "model.encoder = get_peft_model(model.encoder, lora_config)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Carica i pesi salvati\n",
        "weights_path = \"./t5eval_contrastive/pytorch_model.bin\"\n",
        "state_dict = torch.load(weights_path, map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"‚úÖ Modello ricaricato con successo!\")\n",
        "\n",
        "# ===========================\n",
        "# 2. FUNZIONE DI PREDIZIONE\n",
        "# ===========================\n",
        "def get_prediction_regression(precontext, sentence, ending, homonym, judged_meaning, example_sentence):\n",
        "    \"\"\"Genera predizione per un singolo esempio\"\"\"\n",
        "    story = f\"{precontext}\\n{sentence}\\n{ending}\"\n",
        "\n",
        "    prompt = f\"\"\"Rate how plausible the meaning is in the context.\n",
        "Answer ONLY with a number between 1 and 5.\n",
        "You may use decimals (e.g., 2.543, 4.032).\n",
        "\n",
        "Story:\n",
        "{story}\n",
        "\n",
        "Target word: {homonym}\n",
        "Sense: {judged_meaning}\n",
        "Example: {example_sentence}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        max_length=320,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"]\n",
        "        )\n",
        "        pred_score = outputs[\"logits\"].item()\n",
        "\n",
        "    # Clamp tra 1 e 5 provo tra 1.99 e 4.01 per giocare con la stdev e ottenere risultati migliori\n",
        "    return max(1.99, min(4.01, pred_score))\n",
        "\n",
        "# ===========================\n",
        "# 3. GENERAZIONE PREDIZIONI\n",
        "# ===========================\n",
        "print(\"\\nüîÆ Generazione predizioni sul test set...\")\n",
        "\n",
        "# Carica test.json\n",
        "test_path = 'dataset/test.json'\n",
        "with open(test_path, \"r\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for sample_id, item in tqdm(test_data.items(), desc=\"Generating predictions\"):\n",
        "    # Gestisci example_sentence mancante\n",
        "    example_s = item.get('example_sentence', '')\n",
        "    if not example_s and 'gloss' in item:\n",
        "        example_s = item['gloss']\n",
        "\n",
        "    # Genera predizione\n",
        "    pred = get_prediction_regression(\n",
        "        precontext=item['precontext'],\n",
        "        sentence=item['sentence'],\n",
        "        ending=item['ending'],\n",
        "        homonym=item['homonym'],\n",
        "        judged_meaning=item['judged_meaning'],\n",
        "        example_sentence=example_s\n",
        "    )\n",
        "\n",
        "    predictions.append({\n",
        "        \"id\": str(sample_id),\n",
        "        \"prediction\": pred\n",
        "    })\n",
        "\n",
        "# ===========================\n",
        "# 4. SALVATAGGIO FILE\n",
        "# ===========================\n",
        "output_file = \"predictions.jsonl\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    for entry in predictions:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "print(f\"\\n‚úÖ File {output_file} creato con successo con {len(predictions)} predizioni!\")\n",
        "print(f\"üìä Range predizioni: [{min(p['prediction'] for p in predictions):.2f}, {max(p['prediction'] for p in predictions):.2f}]\")\n",
        "\n",
        "# Mostra prime 5 predizioni come verifica\n",
        "print(\"\\nüîç Prime 5 predizioni:\")\n",
        "for p in predictions[:5]:\n",
        "    print(f\"  ID: {p['id']}, Prediction: {p['prediction']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ypW6rpKA0eQS",
        "outputId": "9f2bda40-9968-414d-f06b-584ab6976556"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: predictions.jsonl (deflated 79%)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_a36fd93e-0986-4434-9b6f-8c18dd31cadc\", \"submit_dev.zip\", 8222)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "!zip -r submit_dev.zip ./predictions.jsonl\n",
        "files.download('submit_dev.zip')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "19f5af85be624530a572b9f771924ae1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36f4bbb8dbb44a6e8e9edb58b8a9b0a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ef7c3b513e5471abe8ca55cbb6fb635": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "724f7d0d1b09424bb0cb6aca69164261": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae9249c57f254b33bf71f368ea911d86",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0c9c6784089401b89459cb4c57c7cb7",
            "value": 2
          }
        },
        "99bc0bdd80eb4b6fb22a34406ce96696": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbbccdfa00014c75bdf00904f020f04f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f6c193e1b31e40c6b068490c427e6b2a",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "9a34c017f7884302b6b1efc94f3a685f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ef7c3b513e5471abe8ca55cbb6fb635",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_36f4bbb8dbb44a6e8e9edb58b8a9b0a2",
            "value": "‚Äá2/2‚Äá[00:00&lt;00:00,‚Äá‚Äá1.72it/s]"
          }
        },
        "ae9249c57f254b33bf71f368ea911d86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbbccdfa00014c75bdf00904f020f04f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0c9c6784089401b89459cb4c57c7cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6c193e1b31e40c6b068490c427e6b2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8c82d5d221f44c4b051a0d4679f31ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99bc0bdd80eb4b6fb22a34406ce96696",
              "IPY_MODEL_724f7d0d1b09424bb0cb6aca69164261",
              "IPY_MODEL_9a34c017f7884302b6b1efc94f3a685f"
            ],
            "layout": "IPY_MODEL_19f5af85be624530a572b9f771924ae1"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
